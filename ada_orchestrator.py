#!/usr/bin/env python3
"""
LearnQwestâ„¢ ADA Orchestrator
============================
The REAL ADA - Agent orchestration system that manages 60+ learning agents
Gets YOU out of the loop!

Created by: LINK & Claude Code
Date: October 24, 2025
Version: 1.0 - LearnQwestâ„¢ Edition
"""

import asyncio
import json
import uuid
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Any
from enum import Enum
from dataclasses import dataclass, asdict
import logging

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - [LearnQwestâ„¢ ADA] - %(levelname)s - %(message)s'
)
logger = logging.getLogger("LearnQwest_ADA")


class TaskStatus(Enum):
    """Task execution states"""
    QUEUED = "queued"
    ANALYZING = "analyzing"
    ROUTING = "routing"
    EXECUTING = "executing"
    COMPLETED = "completed"
    FAILED = "failed"
    RETRYING = "retrying"


class TaskPriority(Enum):
    """Task priority levels"""
    LOW = 1
    NORMAL = 2
    HIGH = 3
    URGENT = 4


class QuestDomain(Enum):
    """Learning domains for quest categorization"""
    MATH = "math"
    READING = "reading"
    SCIENCE = "science"
    SOCIAL_STUDIES = "social_studies"
    CREATIVE = "creative"
    GENERAL = "general"


@dataclass
class LearningTask:
    """Represents a learning task (student work to analyze)"""
    id: str
    description: str
    domain: QuestDomain
    priority: TaskPriority
    status: TaskStatus
    created_at: str
    updated_at: str
    student_id: Optional[str] = None
    grade_level: Optional[int] = None
    file_path: Optional[str] = None
    metadata: Dict[str, Any] = None

    # Execution tracking
    assigned_agents: List[str] = None
    execution_plan: Dict[str, Any] = None
    progress: int = 0  # 0-100%

    # Results
    results: Dict[str, Any] = None
    error: Optional[str] = None
    retry_count: int = 0

    def __post_init__(self):
        if self.assigned_agents is None:
            self.assigned_agents = []
        if self.metadata is None:
            self.metadata = {}
        if self.results is None:
            self.results = {}


@dataclass
class AgentExecutionResult:
    """Result from an agent execution"""
    agent_name: str
    status: str
    output: Any
    execution_time: float
    cost: float = 0.0
    error: Optional[str] = None


# =============================================================================
# NEW: ROUTING SPINE - Content-type to Agent matching
# =============================================================================

class ContentType(Enum):
    """Content types that ADA can route"""
    YOUTUBE_VIDEO = "youtube_video"
    CODE_FILE = "code_file"
    DOCUMENT = "document"
    QUESTION = "question"
    SEARCH_QUERY = "search_query"
    IMAGE = "image"
    UNKNOWN = "unknown"


class AgentCapability(Enum):
    """What agents can do"""
    SEARCH = "search"
    ANALYZE = "analyze"
    GENERATE = "generate"
    ASSESS = "assess"
    REFACTOR = "refactor"
    EXPLAIN = "explain"


@dataclass
class AgentRoute:
    """Routing definition for an agent"""
    agent_name: str
    handles_content: List[ContentType]
    capabilities: List[AgentCapability]
    priority: int = 1  # Higher = preferred
    success_rate: float = 1.0  # Learned over time


class RoutingSpine:
    """
    Routes content to the right agent(s) based on content type and intent.

    The spine learns over time which agents perform best for which tasks.
    """

    def __init__(self):
        self.routes: List[AgentRoute] = []
        self._register_default_routes()
        logger.info("RoutingSpine initialized with default routes")

    def _register_default_routes(self):
        """Register default agent routes"""
        # Omnisearch Ion - handles search queries
        self.register_route(AgentRoute(
            agent_name="omnisearch",
            handles_content=[ContentType.SEARCH_QUERY, ContentType.QUESTION],
            capabilities=[AgentCapability.SEARCH],
            priority=10
        ))

        # Quality Assessor - analyzes content quality
        self.register_route(AgentRoute(
            agent_name="quality-assessor",
            handles_content=[ContentType.DOCUMENT, ContentType.CODE_FILE],
            capabilities=[AgentCapability.ASSESS, AgentCapability.ANALYZE],
            priority=8
        ))

        # Code analysis agents
        self.register_route(AgentRoute(
            agent_name="duplicate-detector-ion",
            handles_content=[ContentType.CODE_FILE],
            capabilities=[AgentCapability.ANALYZE],
            priority=5
        ))

        self.register_route(AgentRoute(
            agent_name="refactor-planner-ion",
            handles_content=[ContentType.CODE_FILE],
            capabilities=[AgentCapability.REFACTOR, AgentCapability.ANALYZE],
            priority=5
        ))

        # Claude-Code for code generation/explanation
        self.register_route(AgentRoute(
            agent_name="claude-code",
            handles_content=[ContentType.CODE_FILE, ContentType.QUESTION],
            capabilities=[AgentCapability.GENERATE, AgentCapability.EXPLAIN, AgentCapability.REFACTOR],
            priority=9
        ))

        # YouTube content handler
        self.register_route(AgentRoute(
            agent_name="youtube-extractor",
            handles_content=[ContentType.YOUTUBE_VIDEO],
            capabilities=[AgentCapability.ANALYZE, AgentCapability.GENERATE],
            priority=10
        ))

    def register_route(self, route: AgentRoute):
        """Register a new agent route"""
        self.routes.append(route)
        logger.debug(f"Registered route: {route.agent_name} for {[c.value for c in route.handles_content]}")

    def detect_content_type(self, content: str) -> ContentType:
        """Detect the type of content from the input"""
        content_lower = content.lower()

        # YouTube detection
        if "youtube.com" in content_lower or "youtu.be" in content_lower:
            return ContentType.YOUTUBE_VIDEO

        # Code file detection
        code_extensions = ['.py', '.ts', '.js', '.tsx', '.jsx', '.java', '.go', '.rs']
        if any(ext in content_lower for ext in code_extensions):
            return ContentType.CODE_FILE

        # Code pattern detection
        code_patterns = ['def ', 'function ', 'class ', 'import ', 'const ', 'let ', 'var ']
        if any(pattern in content for pattern in code_patterns):
            return ContentType.CODE_FILE

        # Search query detection
        search_words = ['find', 'search', 'look for', 'where is', 'how to find']
        if any(word in content_lower for word in search_words):
            return ContentType.SEARCH_QUERY

        # Question detection
        if content.strip().endswith('?') or content_lower.startswith(('what', 'how', 'why', 'when', 'where', 'who')):
            return ContentType.QUESTION

        # Document detection
        if len(content) > 500:
            return ContentType.DOCUMENT

        return ContentType.UNKNOWN

    def detect_required_capability(self, content: str) -> AgentCapability:
        """Detect what capability is needed"""
        content_lower = content.lower()

        if any(word in content_lower for word in ['search', 'find', 'look for']):
            return AgentCapability.SEARCH
        if any(word in content_lower for word in ['analyze', 'assess', 'evaluate', 'review']):
            return AgentCapability.ANALYZE
        if any(word in content_lower for word in ['generate', 'create', 'write', 'build', 'make']):
            return AgentCapability.GENERATE
        if any(word in content_lower for word in ['refactor', 'improve', 'optimize', 'fix']):
            return AgentCapability.REFACTOR
        if any(word in content_lower for word in ['explain', 'what is', 'how does', 'why']):
            return AgentCapability.EXPLAIN

        return AgentCapability.ANALYZE  # Default

    def route(self, content: str) -> List[str]:
        """
        Route content to appropriate agent(s).
        Returns list of agent names in priority order.
        """
        content_type = self.detect_content_type(content)
        capability = self.detect_required_capability(content)

        logger.info(f"Routing: content_type={content_type.value}, capability={capability.value}")

        # Find matching routes
        matches = []
        for route in self.routes:
            type_match = content_type in route.handles_content
            capability_match = capability in route.capabilities

            if type_match or capability_match:
                score = route.priority * route.success_rate
                if type_match and capability_match:
                    score *= 2  # Bonus for both matching
                matches.append((route.agent_name, score))

        # Sort by score and return agent names
        matches.sort(key=lambda x: x[1], reverse=True)
        agents = [m[0] for m in matches]

        if not agents:
            agents = ["general-learning-agent"]  # Fallback

        logger.info(f"Routed to agents: {agents}")
        return agents

    def update_success_rate(self, agent_name: str, success: bool):
        """Update agent success rate based on execution outcome"""
        for route in self.routes:
            if route.agent_name == agent_name:
                # Exponential moving average
                alpha = 0.1
                route.success_rate = alpha * (1.0 if success else 0.0) + (1 - alpha) * route.success_rate
                logger.debug(f"Updated {agent_name} success_rate to {route.success_rate:.2f}")
                break


# =============================================================================
# NEW: FEEDBACK LOOP - Learning from execution outcomes
# =============================================================================

@dataclass
class FeedbackEntry:
    """A single feedback entry for learning"""
    timestamp: str
    task_id: str
    content_type: str
    agents_used: List[str]
    success: bool
    execution_time_ms: float
    user_rating: Optional[int] = None  # 1-5 if provided
    notes: Optional[str] = None


class FeedbackLoop:
    """
    Captures feedback from task execution to improve future routing.

    Implements Iron Sharpens Iron learning.
    """

    def __init__(self, feedback_file: Optional[Path] = None):
        self.feedback_file = feedback_file or Path("ada_feedback.jsonl")
        self.entries: List[FeedbackEntry] = []
        self._load_history()
        logger.info(f"FeedbackLoop initialized with {len(self.entries)} historical entries")

    def _load_history(self):
        """Load feedback history from file"""
        if self.feedback_file.exists():
            try:
                with open(self.feedback_file, 'r') as f:
                    for line in f:
                        if line.strip():
                            data = json.loads(line)
                            self.entries.append(FeedbackEntry(**data))
            except Exception as e:
                logger.warning(f"Could not load feedback history: {e}")

    def record(
        self,
        task_id: str,
        content_type: str,
        agents_used: List[str],
        success: bool,
        execution_time_ms: float,
        user_rating: Optional[int] = None,
        notes: Optional[str] = None
    ):
        """Record feedback from a task execution"""
        entry = FeedbackEntry(
            timestamp=datetime.now().isoformat(),
            task_id=task_id,
            content_type=content_type,
            agents_used=agents_used,
            success=success,
            execution_time_ms=execution_time_ms,
            user_rating=user_rating,
            notes=notes
        )

        self.entries.append(entry)

        # Append to file
        try:
            with open(self.feedback_file, 'a') as f:
                f.write(json.dumps(asdict(entry)) + '\n')
        except Exception as e:
            logger.error(f"Could not write feedback: {e}")

        logger.info(f"Recorded feedback for task {task_id}: success={success}")

    def get_agent_stats(self, agent_name: str) -> Dict[str, Any]:
        """Get performance stats for an agent"""
        agent_entries = [e for e in self.entries if agent_name in e.agents_used]

        if not agent_entries:
            return {"total": 0, "success_rate": 0.0, "avg_time_ms": 0.0}

        successes = sum(1 for e in agent_entries if e.success)
        total_time = sum(e.execution_time_ms for e in agent_entries)

        return {
            "total": len(agent_entries),
            "success_rate": successes / len(agent_entries),
            "avg_time_ms": total_time / len(agent_entries),
            "avg_rating": sum(e.user_rating for e in agent_entries if e.user_rating) / max(1, sum(1 for e in agent_entries if e.user_rating))
        }

    def suggest_improvements(self) -> List[str]:
        """Analyze feedback and suggest improvements"""
        suggestions = []

        # Find underperforming agents
        agent_names = set()
        for entry in self.entries:
            agent_names.update(entry.agents_used)

        for agent in agent_names:
            stats = self.get_agent_stats(agent)
            if stats["total"] >= 5 and stats["success_rate"] < 0.7:
                suggestions.append(f"Agent '{agent}' has low success rate ({stats['success_rate']:.0%}). Consider reviewing or replacing.")
            if stats["avg_time_ms"] > 10000:
                suggestions.append(f"Agent '{agent}' is slow (avg {stats['avg_time_ms']:.0f}ms). Consider optimization.")

        return suggestions


# =============================================================================
# NEW: JSONL LOGGER - Structured logging for analytics
# =============================================================================

class JSONLLogger:
    """
    Structured JSONL logging for all ADA operations.

    Every action is logged with full context for debugging and analytics.
    """

    def __init__(self, log_dir: Optional[Path] = None):
        self.log_dir = log_dir or Path("ada_logs")
        self.log_dir.mkdir(exist_ok=True)
        self.session_id = str(uuid.uuid4())[:8]
        self.log_file = self.log_dir / f"ada_{datetime.now().strftime('%Y%m%d')}_{self.session_id}.jsonl"
        logger.info(f"JSONLLogger writing to {self.log_file}")

    def _write(self, event_type: str, data: Dict[str, Any]):
        """Write a log entry"""
        entry = {
            "timestamp": datetime.now().isoformat(),
            "session_id": self.session_id,
            "event_type": event_type,
            **data
        }

        try:
            with open(self.log_file, 'a') as f:
                f.write(json.dumps(entry) + '\n')
        except Exception as e:
            logger.error(f"Log write failed: {e}")

    def task_submitted(self, task_id: str, description: str, domain: str):
        """Log task submission"""
        self._write("task_submitted", {
            "task_id": task_id,
            "description": description[:200],
            "domain": domain
        })

    def routing_decision(self, task_id: str, content_type: str, agents: List[str]):
        """Log routing decision"""
        self._write("routing_decision", {
            "task_id": task_id,
            "content_type": content_type,
            "agents_selected": agents
        })

    def agent_execution(self, task_id: str, agent_name: str, success: bool, time_ms: float, error: Optional[str] = None):
        """Log agent execution"""
        self._write("agent_execution", {
            "task_id": task_id,
            "agent_name": agent_name,
            "success": success,
            "execution_time_ms": time_ms,
            "error": error
        })

    def task_completed(self, task_id: str, success: bool, total_time_ms: float):
        """Log task completion"""
        self._write("task_completed", {
            "task_id": task_id,
            "success": success,
            "total_time_ms": total_time_ms
        })

    def get_session_stats(self) -> Dict[str, Any]:
        """Get stats for current session"""
        stats = {"tasks": 0, "successes": 0, "total_time_ms": 0}

        if self.log_file.exists():
            with open(self.log_file, 'r') as f:
                for line in f:
                    if line.strip():
                        entry = json.loads(line)
                        if entry["event_type"] == "task_completed":
                            stats["tasks"] += 1
                            if entry.get("success"):
                                stats["successes"] += 1
                            stats["total_time_ms"] += entry.get("total_time_ms", 0)

        return stats


class TaskQueue:
    """
    Async task queue with priority scheduling
    Manages concurrent execution limits
    """

    def __init__(self, max_concurrent: int = 5):
        self.queue: List[LearningTask] = []
        self.active_tasks: Dict[str, LearningTask] = {}
        self.completed_tasks: Dict[str, LearningTask] = {}
        self.max_concurrent = max_concurrent
        self._lock = asyncio.Lock()

    async def add_task(self, task: LearningTask) -> str:
        """Add task to queue"""
        async with self._lock:
            self.queue.append(task)
            # Sort by priority (higher priority first)
            self.queue.sort(key=lambda t: t.priority.value, reverse=True)
            logger.info(f"âœ… Task queued: {task.id} (Priority: {task.priority.value})")
            return task.id

    async def get_next_task(self) -> Optional[LearningTask]:
        """Get next task if we have capacity"""
        async with self._lock:
            if len(self.active_tasks) >= self.max_concurrent:
                return None

            if not self.queue:
                return None

            task = self.queue.pop(0)
            self.active_tasks[task.id] = task
            return task

    async def complete_task(self, task_id: str, results: Dict[str, Any]):
        """Mark task as completed"""
        async with self._lock:
            if task_id in self.active_tasks:
                task = self.active_tasks.pop(task_id)
                task.status = TaskStatus.COMPLETED
                task.results = results
                task.updated_at = datetime.now().isoformat()
                task.progress = 100
                self.completed_tasks[task_id] = task
                logger.info(f"âœ… Task completed: {task_id}")

    async def fail_task(self, task_id: str, error: str):
        """Mark task as failed"""
        async with self._lock:
            if task_id in self.active_tasks:
                task = self.active_tasks.pop(task_id)
                task.status = TaskStatus.FAILED
                task.error = error
                task.updated_at = datetime.now().isoformat()
                self.completed_tasks[task_id] = task
                logger.error(f"âŒ Task failed: {task_id} - {error}")

    def get_status(self) -> Dict[str, Any]:
        """Get queue status"""
        return {
            "queued": len(self.queue),
            "active": len(self.active_tasks),
            "completed": len(self.completed_tasks),
            "capacity": self.max_concurrent - len(self.active_tasks)
        }


class TaskAnalyzer:
    """
    Analyzes learning tasks to determine:
    - Complexity level
    - Required agent capabilities
    - Optimal agent team
    """

    def analyze(self, task: LearningTask) -> Dict[str, Any]:
        """Analyze task and determine requirements"""

        description = task.description.lower()

        # Determine complexity
        complexity = self._assess_complexity(description)

        # Determine required capabilities
        capabilities = self._identify_capabilities(description, task.domain)

        # Suggest agent team
        agent_team = self._suggest_agents(task.domain, complexity, capabilities)

        analysis = {
            "complexity": complexity,
            "capabilities_needed": capabilities,
            "suggested_agents": agent_team,
            "estimated_time": self._estimate_time(complexity),
            "estimated_cost": self._estimate_cost(complexity, len(agent_team))
        }

        logger.info(f"ğŸ“Š Task analyzed: {task.id} - Complexity: {complexity}, Agents: {len(agent_team)}")

        return analysis

    def _assess_complexity(self, description: str) -> str:
        """Assess task complexity"""
        complex_keywords = ['analyze', 'comprehensive', 'detailed', 'multiple', 'all']
        medium_keywords = ['explain', 'create', 'generate', 'process']

        if any(keyword in description for keyword in complex_keywords):
            return "complex"
        elif any(keyword in description for keyword in medium_keywords):
            return "medium"
        else:
            return "simple"

    def _identify_capabilities(self, description: str, domain: QuestDomain) -> List[str]:
        """Identify required agent capabilities"""
        capabilities = []

        # Domain-specific capabilities
        if domain == QuestDomain.MATH:
            if 'algebra' in description:
                capabilities.append('algebra_specialist')
            if 'geometry' in description:
                capabilities.append('geometry_specialist')
            if 'word problem' in description:
                capabilities.append('problem_solving')

        # General capabilities
        if 'feedback' in description or 'assess' in description:
            capabilities.append('assessment')
        if 'generate' in description or 'create' in description:
            capabilities.append('content_generation')
        if 'analyze' in description:
            capabilities.append('analysis')

        return capabilities if capabilities else ['general']

    def _suggest_agents(self, domain: QuestDomain, complexity: str, capabilities: List[str]) -> List[str]:
        """Suggest optimal agent team"""

        # Base agents by domain
        agent_map = {
            QuestDomain.MATH: ['math-quest-agent', 'problem-solver-agent', 'feedback-generator'],
            QuestDomain.READING: ['reading-comprehension-agent', 'analysis-agent'],
            QuestDomain.SCIENCE: ['science-quest-agent', 'experiment-analyzer'],
            QuestDomain.SOCIAL_STUDIES: ['history-agent', 'geography-agent'],
            QuestDomain.CREATIVE: ['creative-writing-agent', 'art-analysis-agent'],
            QuestDomain.GENERAL: ['general-learning-agent']
        }

        agents = agent_map.get(domain, ['general-learning-agent']).copy()

        # Add complexity-based agents
        if complexity == 'complex':
            agents.extend(['advanced-analyzer', 'quality-validator'])

        # Add capability-specific agents
        if 'assessment' in capabilities:
            agents.append('assessment-specialist')
        if 'content_generation' in capabilities:
            agents.append('content-generator')

        return agents

    def _estimate_time(self, complexity: str) -> int:
        """Estimate execution time in seconds"""
        time_map = {
            'simple': 3,
            'medium': 5,
            'complex': 10
        }
        return time_map.get(complexity, 5)

    def _estimate_cost(self, complexity: str, agent_count: int) -> float:
        """Estimate execution cost in dollars"""
        cost_per_agent = {
            'simple': 0.01,
            'medium': 0.03,
            'complex': 0.05
        }
        return cost_per_agent.get(complexity, 0.03) * agent_count


class ExecutionMonitor:
    """
    Real-time monitoring of task execution
    Tracks progress, logs activity, detects failures
    """

    def __init__(self):
        self.metrics = {
            "total_tasks": 0,
            "successful_tasks": 0,
            "failed_tasks": 0,
            "total_cost": 0.0,
            "total_execution_time": 0.0,
            "agent_usage": {}
        }
        self.active_monitors = {}

    def start_monitoring(self, task_id: str):
        """Start monitoring a task"""
        self.active_monitors[task_id] = {
            "start_time": datetime.now(),
            "status_log": []
        }
        self.metrics["total_tasks"] += 1
        logger.info(f"ğŸ” Started monitoring task: {task_id}")

    def log_progress(self, task_id: str, progress: int, message: str):
        """Log task progress"""
        if task_id in self.active_monitors:
            self.active_monitors[task_id]["status_log"].append({
                "timestamp": datetime.now().isoformat(),
                "progress": progress,
                "message": message
            })
            logger.info(f"ğŸ“Š Task {task_id}: {progress}% - {message}")

    def log_agent_execution(self, task_id: str, agent_name: str, result: AgentExecutionResult):
        """Log agent execution result"""
        # Update agent usage metrics
        if agent_name not in self.metrics["agent_usage"]:
            self.metrics["agent_usage"][agent_name] = {
                "executions": 0,
                "total_time": 0.0,
                "total_cost": 0.0
            }

        self.metrics["agent_usage"][agent_name]["executions"] += 1
        self.metrics["agent_usage"][agent_name]["total_time"] += result.execution_time
        self.metrics["agent_usage"][agent_name]["total_cost"] += result.cost

        self.metrics["total_cost"] += result.cost
        self.metrics["total_execution_time"] += result.execution_time

    def complete_monitoring(self, task_id: str, success: bool):
        """Complete monitoring for a task"""
        if task_id in self.active_monitors:
            monitor_data = self.active_monitors.pop(task_id)
            execution_time = (datetime.now() - monitor_data["start_time"]).total_seconds()

            if success:
                self.metrics["successful_tasks"] += 1
                logger.info(f"âœ… Task {task_id} completed in {execution_time:.2f}s")
            else:
                self.metrics["failed_tasks"] += 1
                logger.error(f"âŒ Task {task_id} failed after {execution_time:.2f}s")

    def get_metrics(self) -> Dict[str, Any]:
        """Get current metrics"""
        success_rate = 0
        if self.metrics["total_tasks"] > 0:
            success_rate = (self.metrics["successful_tasks"] / self.metrics["total_tasks"]) * 100

        return {
            **self.metrics,
            "success_rate": f"{success_rate:.1f}%",
            "average_cost": self.metrics["total_cost"] / max(self.metrics["total_tasks"], 1),
            "average_time": self.metrics["total_execution_time"] / max(self.metrics["total_tasks"], 1)
        }


class ADAOrchestrator:
    """
    The REAL ADA - LearnQwestâ„¢ Agent Orchestration System

    Manages 60+ learning agents to:
    - Analyze student work
    - Generate personalized feedback
    - Create adaptive learning paths
    - Get teachers OUT of the loop!
    """

    def __init__(self, max_concurrent: int = 5):
        logger.info("Initializing LearnQwest ADA Orchestrator...")

        # Core components
        self.task_queue = TaskQueue(max_concurrent=max_concurrent)
        self.task_analyzer = TaskAnalyzer()
        self.monitor = ExecutionMonitor()

        # NEW: Routing, Feedback, and Logging
        self.routing_spine = RoutingSpine()
        self.feedback_loop = FeedbackLoop()
        self.jsonl_logger = JSONLLogger()

        # Load RAGEFORCE infrastructure (will be migrated to LearnQwest)
        self.load_infrastructure()

        # Start task processor
        self._running = False
        self._processor_task = None

        logger.info("ADA Orchestrator ready!")
        logger.info(f"Max concurrent tasks: {max_concurrent}")
        logger.info(f"Available agents: {self.routing_spine.list_ions() if hasattr(self.routing_spine, 'list_ions') else len(self.routing_spine.routes)}")

    def load_infrastructure(self):
        """Load existing RAGEFORCE infrastructure"""
        try:
            # Try to import existing components
            import sys
            sys.path.insert(0, r'C:\Users\talon\OneDrive\Projects\RAGEFORCE')

            try:
                from ragents.AGENT_WRANGLER import AgentWrangler
                self.agent_wrangler = AgentWrangler()
                logger.info("âœ… Agent Wrangler loaded (60+ agents available)")
            except Exception as e:
                logger.warning(f"âš ï¸ Agent Wrangler not available: {e}")
                self.agent_wrangler = None

            try:
                from scout_plan_build_orchestrator import ScoutPlanBuildOrchestrator
                self.spb_orchestrator = ScoutPlanBuildOrchestrator()
                logger.info("âœ… Scout-Plan-Build Orchestrator loaded")
            except Exception as e:
                logger.warning(f"âš ï¸ SPB Orchestrator not available: {e}")
                self.spb_orchestrator = None

        except Exception as e:
            logger.warning(f"âš ï¸ RAGEFORCE infrastructure not fully loaded: {e}")
            logger.info("ğŸ’¡ Running in standalone mode with simulated agents")
            self.agent_wrangler = None
            self.spb_orchestrator = None

    async def submit_task(
        self,
        description: str,
        domain: QuestDomain = QuestDomain.GENERAL,
        priority: TaskPriority = TaskPriority.NORMAL,
        student_id: Optional[str] = None,
        grade_level: Optional[int] = None,
        file_path: Optional[str] = None,
        metadata: Optional[Dict[str, Any]] = None
    ) -> str:
        """
        Submit a learning task for processing

        Returns task_id for status tracking
        """

        task = LearningTask(
            id=str(uuid.uuid4()),
            description=description,
            domain=domain,
            priority=priority,
            status=TaskStatus.QUEUED,
            created_at=datetime.now().isoformat(),
            updated_at=datetime.now().isoformat(),
            student_id=student_id,
            grade_level=grade_level,
            file_path=file_path,
            metadata=metadata or {}
        )

        task_id = await self.task_queue.add_task(task)

        logger.info(f"ğŸ“ Task submitted: {task_id}")
        logger.info(f"   Description: {description}")
        logger.info(f"   Domain: {domain.value}")
        logger.info(f"   Priority: {priority.value}")

        return task_id

    async def process_task(self, task: LearningTask):
        """Process a single task through the complete workflow"""
        import time
        start_time = time.time()

        self.monitor.start_monitoring(task.id)
        self.jsonl_logger.task_submitted(task.id, task.description, task.domain.value)

        try:
            # PHASE 1: ANALYZE
            task.status = TaskStatus.ANALYZING
            task.updated_at = datetime.now().isoformat()
            self.monitor.log_progress(task.id, 10, "Analyzing task requirements")

            analysis = self.task_analyzer.analyze(task)
            task.execution_plan = analysis

            # PHASE 2: ROUTE (NEW - using RoutingSpine)
            task.status = TaskStatus.ROUTING
            task.updated_at = datetime.now().isoformat()

            # Use RoutingSpine for intelligent routing
            content_type = self.routing_spine.detect_content_type(task.description)
            routed_agents = self.routing_spine.route(task.description)
            task.assigned_agents = routed_agents[:5]  # Top 5 agents

            self.jsonl_logger.routing_decision(task.id, content_type.value, task.assigned_agents)
            self.monitor.log_progress(task.id, 30, f"Routed to {len(task.assigned_agents)} agents: {task.assigned_agents}")

            # Fallback to Agent Wrangler if available and routing returned generic
            if self.agent_wrangler and "general-learning-agent" in task.assigned_agents:
                team, plan = self.agent_wrangler.quick_task(task.description)
                if team and plan:
                    task.assigned_agents = team.agents[:5]
                    task.execution_plan.update({"wrangler_plan": plan})
                    self.monitor.log_progress(task.id, 40, "Agent team enhanced via Wrangler")

            # PHASE 3: EXECUTE
            task.status = TaskStatus.EXECUTING
            task.updated_at = datetime.now().isoformat()
            self.monitor.log_progress(task.id, 50, "Executing with agent team")

            results = await self.execute_with_agents(task)

            # PHASE 4: COMPLETE
            task.results = results
            task.progress = 100
            await self.task_queue.complete_task(task.id, results)
            self.monitor.complete_monitoring(task.id, success=True)

            # NEW: Record feedback for learning
            total_time_ms = (time.time() - start_time) * 1000
            self.feedback_loop.record(
                task_id=task.id,
                content_type=content_type.value,
                agents_used=task.assigned_agents,
                success=True,
                execution_time_ms=total_time_ms
            )
            self.jsonl_logger.task_completed(task.id, success=True, total_time_ms=total_time_ms)

            # Update routing spine with success
            for agent in task.assigned_agents:
                self.routing_spine.update_success_rate(agent, success=True)

            logger.info(f"Task {task.id} completed successfully in {total_time_ms:.0f}ms!")

        except Exception as e:
            total_time_ms = (time.time() - start_time) * 1000
            logger.error(f"Task {task.id} failed: {e}")
            await self.task_queue.fail_task(task.id, str(e))
            self.monitor.complete_monitoring(task.id, success=False)

            # Record failure feedback
            self.feedback_loop.record(
                task_id=task.id,
                content_type="unknown",
                agents_used=task.assigned_agents,
                success=False,
                execution_time_ms=total_time_ms,
                notes=str(e)
            )
            self.jsonl_logger.task_completed(task.id, success=False, total_time_ms=total_time_ms)

            # Update routing spine with failure
            for agent in task.assigned_agents:
                self.routing_spine.update_success_rate(agent, success=False)

    async def execute_with_agents(self, task: LearningTask) -> Dict[str, Any]:
        """Execute task with assigned agents"""
        import time

        results = {
            "task_id": task.id,
            "status": "success",
            "agents_used": task.assigned_agents,
            "outputs": [],
            "timestamp": datetime.now().isoformat()
        }

        # Execute each agent
        for i, agent_name in enumerate(task.assigned_agents):
            agent_start = time.time()
            self.monitor.log_progress(task.id, 50 + (i * 10), f"Executing {agent_name}")

            try:
                # TODO: Replace with real agent execution via IonBridge
                # For now, simulate agent work
                await asyncio.sleep(0.3)
                output = f"Result from {agent_name} for: {task.description[:50]}"
                success = True
                error = None

            except Exception as e:
                output = None
                success = False
                error = str(e)

            agent_time_ms = (time.time() - agent_start) * 1000

            agent_result = AgentExecutionResult(
                agent_name=agent_name,
                status="success" if success else "failed",
                output=output,
                execution_time=agent_time_ms / 1000,
                cost=0.01,
                error=error
            )

            # Log to all systems
            self.monitor.log_agent_execution(task.id, agent_name, agent_result)
            self.jsonl_logger.agent_execution(task.id, agent_name, success, agent_time_ms, error)
            results["outputs"].append(asdict(agent_result))

        # Generate learning feedback
        results["feedback"] = self.generate_learning_feedback(task, results)

        return results

    def generate_learning_feedback(self, task: LearningTask, results: Dict[str, Any]) -> Dict[str, Any]:
        """Generate personalized learning feedback"""

        return {
            "overall_assessment": "Good progress! Here's what we found:",
            "strengths": [
                "Clear problem-solving approach",
                "Shows understanding of core concepts"
            ],
            "areas_for_growth": [
                "Double-check calculations in final steps",
                "Add more detailed explanations"
            ],
            "next_steps": [
                "Practice similar problems with different numbers",
                "Try the advanced challenge problems"
            ],
            "personalized_resources": [
                "Video: Understanding place value",
                "Exercise: Interactive practice problems"
            ],
            "teacher_insights": f"Student shows mastery of basic concepts. Ready for more challenging work."
        }

    async def start(self):
        """Start the orchestrator task processor"""
        if self._running:
            logger.warning("âš ï¸ Orchestrator already running")
            return

        self._running = True
        self._processor_task = asyncio.create_task(self._process_queue())
        logger.info("ğŸš€ Orchestrator started - processing tasks")

    async def stop(self):
        """Stop the orchestrator"""
        self._running = False
        if self._processor_task:
            self._processor_task.cancel()
            try:
                await self._processor_task
            except asyncio.CancelledError:
                pass
        logger.info("ğŸ›‘ Orchestrator stopped")

    async def _process_queue(self):
        """Main task processing loop"""
        while self._running:
            try:
                # Get next task
                task = await self.task_queue.get_next_task()

                if task:
                    # Process task in background
                    asyncio.create_task(self.process_task(task))
                else:
                    # No tasks available, wait a bit
                    await asyncio.sleep(1)

            except Exception as e:
                logger.error(f"Error in task processor: {e}")
                await asyncio.sleep(1)

    def get_task_status(self, task_id: str) -> Optional[Dict[str, Any]]:
        """Get status of a specific task"""

        # Check active tasks
        if task_id in self.task_queue.active_tasks:
            task = self.task_queue.active_tasks[task_id]
            return self._task_to_dict(task)

        # Check completed tasks
        if task_id in self.task_queue.completed_tasks:
            task = self.task_queue.completed_tasks[task_id]
            return self._task_to_dict(task)

        # Check queued tasks
        for task in self.task_queue.queue:
            if task.id == task_id:
                return self._task_to_dict(task)

        return None

    def _task_to_dict(self, task: LearningTask) -> Dict[str, Any]:
        """Convert task to dictionary"""
        return {
            "id": task.id,
            "description": task.description,
            "domain": task.domain.value,
            "status": task.status.value,
            "priority": task.priority.value,
            "progress": task.progress,
            "created_at": task.created_at,
            "updated_at": task.updated_at,
            "assigned_agents": task.assigned_agents,
            "results": task.results,
            "error": task.error
        }

    def get_system_status(self) -> Dict[str, Any]:
        """Get complete system status"""
        return {
            "orchestrator": "LearnQwestâ„¢ ADA",
            "status": "running" if self._running else "stopped",
            "queue": self.task_queue.get_status(),
            "metrics": self.monitor.get_metrics(),
            "infrastructure": {
                "agent_wrangler": self.agent_wrangler is not None,
                "spb_orchestrator": self.spb_orchestrator is not None,
                "total_agents": 60 if self.agent_wrangler else 0
            }
        }


# CLI Interface
async def main():
    """Main CLI interface for testing"""

    print("\n" + "="*70)
    print("ğŸ¯ LearnQwestâ„¢ ADA Orchestrator - Command Line Interface")
    print("="*70)

    # Initialize orchestrator
    ada = ADAOrchestrator(max_concurrent=3)
    await ada.start()

    print("\nâœ… ADA ready! Submit tasks or type 'status' to see system info\n")

    try:
        while True:
            print("-" * 70)
            print("Commands:")
            print("  1. Submit task")
            print("  2. Check task status")
            print("  3. System status")
            print("  4. Exit")
            print("-" * 70)

            choice = input("\nChoice (1-4): ").strip()

            if choice == '1':
                description = input("\nTask description: ").strip()
                if description:
                    task_id = await ada.submit_task(
                        description=description,
                        domain=QuestDomain.MATH,
                        priority=TaskPriority.NORMAL
                    )
                    print(f"\nâœ… Task submitted! ID: {task_id}")

                    # Wait a bit and show status
                    await asyncio.sleep(2)
                    status = ada.get_task_status(task_id)
                    if status:
                        print(f"\nCurrent status: {status['status']} ({status['progress']}%)")

            elif choice == '2':
                task_id = input("\nTask ID: ").strip()
                status = ada.get_task_status(task_id)
                if status:
                    print(f"\nğŸ“Š Task Status:")
                    print(json.dumps(status, indent=2))
                else:
                    print("\nâŒ Task not found")

            elif choice == '3':
                status = ada.get_system_status()
                print(f"\nğŸ“Š System Status:")
                print(json.dumps(status, indent=2))

            elif choice == '4':
                print("\nğŸ‘‹ Shutting down ADA...")
                break

            await asyncio.sleep(0.1)

    finally:
        await ada.stop()
        print("\nâœ… ADA Orchestrator stopped. Have a great day!")


if __name__ == "__main__":
    print("\nğŸš€ Starting LearnQwestâ„¢ ADA Orchestrator...\n")
    asyncio.run(main())
